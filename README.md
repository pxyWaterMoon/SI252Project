# [SI252 Project] *Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning*

Members: Liyu Yang; Lingkai Zu; Xiyue Peng

*Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning* presents a rule-based reinforcement learning framework that acquires R1-like reasoning patterns through training on logic puzzles. 
Following the 7 research questions mentioned in this paper:
+ How Does GRPO Compare to Other RL Algorithms?
+ Do certain thinking tokens and language-mixing phenomena improve reasoning?
+ Does an ’Aha Moment’ Emerge During Training?
+ Can the Model Generalize to Out-of-Distribution (OOD) Tasks?
+ Which Generalizes Better, SFT or RL?
+ Does Longer Response Length Guarantee Better Reasoning?
+ Is Curriculum Learning Still Necessary in RL?
+ Can similar reasoning abilities emerge in smaller-scale models?

We try to find the answer to "Can similar reasoning abilities emerge in smaller-scale models?" and "What is the optimal training data structure for fostering such capabilities?" in this projected.

## Project Plan
+ [ ] Make a review of the existing post-training algorithm and compare its performance.
+ [ ] Under the condition of sufficient computational resources, reproduce the 7 research questions with a suitable scale LMs.
+ [ ] Combining the findings from the article with our experimental results presents insights and reflections on improving the performance of LMs during post-training.
